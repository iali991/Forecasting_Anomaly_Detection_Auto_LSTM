{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anomaly detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "import math\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import optimizers, Sequential\n",
    "from keras.models import Model\n",
    "from keras.utils import plot_model\n",
    "from keras.layers import Dense, LSTM, RepeatVector, TimeDistributed\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "import numpy as np\n",
    "from numpy import arange, sin, pi, random\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import scipy.integrate as integrate\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_curve\n",
    "from sklearn.metrics import recall_score, classification_report, auc, roc_curve\n",
    "from sklearn.metrics import precision_recall_fscore_support, f1_score\n",
    "from sklearn.neighbors.kde import KernelDensity\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "np.random.seed(1234)  \n",
    "PYTHONHASHSEED = 0\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import confusion_matrix, recall_score, precision_score\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, LSTM, Activation\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Train and Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_wave():\n",
    "    \"\"\" Generate a synthetic data\n",
    "    :return: the final data\n",
    "    \"\"\"\n",
    "    t = np.arange(0.0, 100.0, 0.01)\n",
    "    wave1 = sin(2 * 2 * pi * t)\n",
    "    noise = random.normal(0, 0.2, len(t))\n",
    "    wave1 = wave1 + noise\n",
    "    print(\"wave1\", len(wave1))\n",
    "    wave2 = sin(2 * pi * t)\n",
    "    print(\"wave2\", len(wave2))\n",
    "    t_rider = arange(0.0, 5.0, 0.01)\n",
    "    wave3 = -2*sin(10 * pi * t_rider)\n",
    "    print(\"wave3\", len(wave3))\n",
    "    insert = round(0.8 * len(t))\n",
    "    wave1[insert:insert + 500] = wave1[insert:insert + 500] + wave3\n",
    "    return wave1 - 2*wave2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def z_norm(result):\n",
    "    result_mean = result.mean()\n",
    "    result_std = result.std()\n",
    "    result -= result_mean\n",
    "    result /= result_std\n",
    "    return result, result_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_split_prep_data(train_start, train_end,\n",
    "                          test_start, test_end):\n",
    "    data = gen_wave()\n",
    "    print(\"Length of Data\", len(data))\n",
    "\n",
    "    # train data\n",
    "    print (\"Creating train data...\")\n",
    "\n",
    "    result = []\n",
    "    for index in range(train_start, train_end - sequence_length):\n",
    "        result.append(data[index: index + sequence_length])\n",
    "    result = np.array(result)  # shape (samples, sequence_length)\n",
    "    result, result_mean = z_norm(result)\n",
    "\n",
    "    print (\"Mean of train data : \", result_mean)\n",
    "    print (\"Train data shape  : \", result.shape)\n",
    "\n",
    "    train = result[train_start:train_end, :]\n",
    "    np.random.shuffle(train)  # shuffles in-place\n",
    "    X_train = train[:, :-1]\n",
    "    y_train = train[:, -1]\n",
    "\n",
    "    # test data\n",
    "    print (\"Creating test data...\")\n",
    "\n",
    "    result = []\n",
    "    for index in range(test_start, test_end - sequence_length):\n",
    "        result.append(data[index: index + sequence_length])\n",
    "    result = np.array(result)  # shape (samples, sequence_length)\n",
    "    result, result_mean = z_norm(result)\n",
    "\n",
    "    print (\"Mean of test data : \", result_mean)\n",
    "    print (\"Test data shape  : \", result.shape)\n",
    "\n",
    "    X_test = result[:, :-1]\n",
    "    y_test = result[:, -1]\n",
    "\n",
    "    print(\"Shape X_train\", np.shape(X_train))\n",
    "    print(\"Shape X_test\", np.shape(X_test))\n",
    "\n",
    "    X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n",
    "    X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\n",
    "\n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(X):\n",
    "    '''\n",
    "    Flatten a 3D array.\n",
    "    \n",
    "    Input\n",
    "    X            A 3D array for lstm, where the array is sample x sequence length x features.\n",
    "    \n",
    "    Output\n",
    "    flattened_X  A 2D array, sample x features.\n",
    "    '''\n",
    "    flattened_X = np.empty((X.shape[0], X.shape[2]))  # sample x features array.\n",
    "    for i in range(X.shape[0]):\n",
    "        flattened_X[i] = X[i, (X.shape[1]-1), :]\n",
    "    return(flattened_X)           \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kde_sklearn(x, bandwidth=0.2, **kwargs):\n",
    "\tx_grid = np.linspace(x.min() - 0.9*x.min(), x.max() + x.max(), 500)\n",
    "\t\"\"\"Kernel Density Estimation with Scikit-learn\"\"\"\n",
    "\tkde_skl = KernelDensity(bandwidth=bandwidth, **kwargs)\n",
    "\tkde_skl.fit(x[:, np.newaxis])\n",
    "\t# score_samples() returns the log-likelihood of the samples\n",
    "\tlog_pdf = kde_skl.score_samples(x_grid[:, np.newaxis])\n",
    "\treturn np.exp(log_pdf), x_grid\n",
    "   \n",
    "def FindThreshold(x,h,p):\n",
    "    tau=0\n",
    "    x.sort() \n",
    "    for i in range(len(x)):\n",
    "        int_K = integrate.quad(lambda s: (1/(h*np.sqrt(2*np.pi)))*np.exp(-0.5*(s-p)/h), (i-1)/len(x), i/len(x))\n",
    "        tau=tau+int_K[0]*x[i]\n",
    "    return tau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# train on first 7000 samples and test on next 3000 samples (has anomaly)\n",
    "sequence_length = 11\n",
    "DATA_SPLIT_PCT = 0.2\n",
    "SEED = 123 #used to help randomly select the data points\n",
    "batch_size = 50\n",
    "epochs = 3\n",
    "X_train,y_train, X_test, y_test = get_split_prep_data(0, 6999, 7000, 10000)\n",
    "X_train_X, X_valid = train_test_split(X_train, test_size=DATA_SPLIT_PCT, random_state=SEED)\n",
    "timesteps =  X_train.shape[1] # equal to the sequence_length\n",
    "n_features =  X_train.shape[2] # 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import keras\n",
    "results = {}\n",
    "for num_cells in [16, 32, 64,80,96]:\n",
    "    for lr in [1e-4, 1e-3, 1e-2]:\n",
    "            print('Running with', num_cells, \n",
    "                  'LSTM cells\n",
    "                  'and learning rate =', lr, '...')\n",
    "\n",
    "            # build network\n",
    "            lstm_autoencoder = Sequential()\n",
    "            # Encoder\n",
    "            lstm_autoencoder.add(LSTM(100, activation='relu', input_shape=(timesteps, n_features), return_sequences=True))\n",
    "            lstm_autoencoder.add(LSTM(25, activation='relu', return_sequences=False))\n",
    "            lstm_autoencoder.add(RepeatVector(timesteps))\n",
    "            # Decoder\n",
    "            lstm_autoencoder.add(LSTM(25, activation='relu', return_sequences=True))\n",
    "            lstm_autoencoder.add(LSTM(100, activation='relu', return_sequences=True))\n",
    "            lstm_autoencoder.add(TimeDistributed(Dense(n_features)))\n",
    "            lstm_autoencoder.summary()\n",
    "            adam = optimizers.Adam(lr)\n",
    "            lstm_autoencoder.compile(loss='mse', optimizer=adam)\n",
    "            cp = ModelCheckpoint(filepath=\"lstm_autoencoder_classifier.h5\",\n",
    "                               save_best_only=True,\n",
    "                               verbose=0)\n",
    "            tb = TensorBoard(log_dir='./logs',\n",
    "                histogram_freq=0,\n",
    "                write_graph=True,\n",
    "                write_images=True)\n",
    "            lstm_autoencoder_history = lstm_autoencoder.fit(X_train, X_train, \n",
    "                                                epochs=epochs, \n",
    "                                                batch_size=batch_size, \n",
    "                                                validation_data=(X_valid, X_valid),\n",
    "                                                verbose=2, callbacks = [keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=0, mode='auto')]).history\n",
    "            print(\"Predicting...\")\n",
    "            predicted_train = lstm_autoencoder.predict(X_train)\n",
    "            predicted = lstm_autoencoder.predict(X_test)\n",
    "            mse = np.mean(np.power(flatten(X_test) - flatten(predicted), 2), axis=1)\n",
    "            mse_train = np.mean(np.power(flatten(X_train) - flatten(predicted_train), 2), axis=1)\n",
    "            #KDE\n",
    "            # use grid search cross-validation to optimize the bandwidth\n",
    "            params = {'bandwidth': np.linspace(0, 0.5, 10)}\n",
    "            grid = GridSearchCV(KernelDensity(), params, cv = 20)\n",
    "            grid.fit(mse_train[:, None])\n",
    "            h=grid.best_estimator_.bandwidth\n",
    "            tau=FindThreshold(mse_train,h,0.56)\n",
    "            y_test1=np.ones(X_test.shape[0])\n",
    "            y_test1[999:1499]=-1\n",
    "            y_scores=np.ones(X_test.shape[0])\n",
    "            y_scores[(mse-tau)>0]=-1\n",
    "            accuracy_kde = accuracy_score(y_test1, y_scores)\n",
    "            #OCSVM\n",
    "            e=X_train - predicted_train\n",
    "            nsamples, nx, ny = e.shape\n",
    "            d2_e = e.reshape((nsamples,nx*ny))\n",
    "            from sklearn import svm\n",
    "            clf = svm.OneClassSVM(nu=0.0055, kernel=\"rbf\", gamma=1.5)\n",
    "            clf.fit(d2_e)\n",
    "            e_t=X_test - predicted\n",
    "            nsamples, nx, ny = e_t.shape\n",
    "            d2_e_t = e_t.reshape((nsamples,nx*ny))\n",
    "            y_scores = clf.predict(d2_e_t)\n",
    "            accuracy_svm = accuracy_score(y_test1, y_scores)\n",
    "            accuracy= min(accuracy_svm,accuracy_kde)\n",
    "            results[(num_cells, lr)] = {'accuracy': accuracy}       \n",
    "val_results = {key: results[key]['accuracy'] for key in results.keys()}\n",
    "num_cells, lr = min(val_results, key=val_results.get)\n",
    "print('Best parameters:', num_cells, \n",
    "        'and learning rate =', lr)\n",
    "\n",
    "#Best parameters: num_cells=64,  lr=0.01\n",
    "#num_cells=64\n",
    "#lr = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Labels for Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import keras\n",
    "lstm_autoencoder = Sequential()\n",
    "\n",
    "# Encoder\n",
    "lstm_autoencoder.add(LSTM(num_cells*4, activation='relu', input_shape=(timesteps, n_features), return_sequences=True))\n",
    "lstm_autoencoder.add(LSTM(num_cells, activation='relu', return_sequences=False))\n",
    "lstm_autoencoder.add(RepeatVector(timesteps))\n",
    "# Decoder\n",
    "lstm_autoencoder.add(LSTM(num_cells, activation='relu', return_sequences=True))\n",
    "lstm_autoencoder.add(LSTM(num_cells*4, activation='relu', return_sequences=True))\n",
    "lstm_autoencoder.add(TimeDistributed(Dense(n_features)))\n",
    "\n",
    "lstm_autoencoder.summary()\n",
    "\n",
    "adam = optimizers.Adam(lr)\n",
    "lstm_autoencoder.compile(loss='mse', optimizer=adam)\n",
    "\n",
    "cp = ModelCheckpoint(filepath=\"lstm_autoencoder_classifier.h5\",\n",
    "                               save_best_only=True,\n",
    "                               verbose=0)\n",
    "\n",
    "tb = TensorBoard(log_dir='./logs',\n",
    "                histogram_freq=0,\n",
    "                write_graph=True,\n",
    "                write_images=True)\n",
    "\n",
    "\n",
    "lstm_autoencoder_history = lstm_autoencoder.fit(X_train, X_train, \n",
    "                                                epochs=epochs, \n",
    "                                                batch_size=batch_size, \n",
    "                                                validation_data=(X_valid, X_valid),\n",
    "                                                verbose=2, callbacks = [keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=0, mode='auto')]).history\n",
    "                                                \n",
    "plt.plot(lstm_autoencoder_history['loss'], linewidth=2, label='Train')\n",
    "plt.plot(lstm_autoencoder_history['val_loss'], linewidth=2, label='Valid')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize Train and Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "print(\"Predicting...\")\n",
    "predicted_train = lstm_autoencoder.predict(X_train)\n",
    "predicted = lstm_autoencoder.predict(X_test)\n",
    "#print(\"Reshaping predicted\")\n",
    "#predicted_train = np.reshape(predicted_train, (predicted_train.size,))\n",
    "#predicted = np.reshape(predicted, (predicted.size,))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"Actual Test Signal w/Anomalies\")\n",
    "plt.plot(y_train[:len(y_train)], 'b')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"Squared Error\")\n",
    "mse = np.mean(np.power(flatten(X_test) - flatten(predicted), 2), axis=1)\n",
    "plt.plot(mse, 'r')\n",
    "mse_train = np.mean(np.power(flatten(X_train) - flatten(predicted_train), 2), axis=1)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.figure()\n",
    "plt.title(\"Actual Test Signal with Anomalies\")\n",
    "plt.plot(y_test[:len(y_test)], 'b')\n",
    "plt.savefig('Epoch.png', dpi=1200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#KDE\n",
    "def kde_sklearn(x, bandwidth=0.2, **kwargs):\n",
    "\tx_grid = np.linspace(x.min() - 0.9*x.min(), x.max() + x.max(), 500)\n",
    "\t\"\"\"Kernel Density Estimation with Scikit-learn\"\"\"\n",
    "\tkde_skl = KernelDensity(bandwidth=bandwidth, **kwargs)\n",
    "\tkde_skl.fit(x[:, np.newaxis])\n",
    "\t# score_samples() returns the log-likelihood of the samples\n",
    "\tlog_pdf = kde_skl.score_samples(x_grid[:, np.newaxis])\n",
    "\treturn np.exp(log_pdf), x_grid\n",
    "   \n",
    "def FindThreshold(x,h,p):\n",
    "    tau=0\n",
    "    x.sort() \n",
    "    for i in range(len(x)):\n",
    "        int_K = integrate.quad(lambda s: (1/(h*np.sqrt(2*np.pi)))*np.exp(-0.5*(s-p)/h), (i-1)/len(x), i/len(x))\n",
    "        tau=tau+int_K[0]*x[i]\n",
    "    return tau\n",
    "\n",
    "#Plot histogram\n",
    "fig, ax2 = plt.subplots(figsize = (9,7))\n",
    "ax2.hist(mse_train, bins = 100, alpha = 0.5, density = True)\n",
    "\n",
    "#Plot KDE\n",
    "pdf, mse_grid = kde_sklearn(mse, bandwidth = 0.5)\n",
    "pdf2, mse_grid2 = kde_sklearn(mse, bandwidth = 0.1)\n",
    "pdf3, mse_grid3 = kde_sklearn(mse, bandwidth = 0.9)\n",
    "ax2.plot(mse_grid, pdf, alpha = 0.9, color = 'green', linewidth = 2.0)\n",
    "ax2.plot(mse_grid2, pdf2, alpha = 0.9, color = 'red', linewidth = 2.0)\n",
    "ax2.plot(mse_grid3, pdf3, alpha = 0.9, color = 'yellow', linewidth = 2.0)\n",
    "plt.show()\n",
    "\n",
    "# use grid search cross-validation to optimize the bandwidth\n",
    "params = {'bandwidth': np.linspace(0, 0.5, 10)}\n",
    "grid = GridSearchCV(KernelDensity(), params, cv = 20)\n",
    "grid.fit(mse_train[:, None])\n",
    "\n",
    "print(\"best bandwidth: {0}\".format(grid.best_estimator_.bandwidth))\n",
    "h=grid.best_estimator_.bandwidth\n",
    "tau=FindThreshold(mse_train,h,0.56)\n",
    "\n",
    "y_test1=np.ones(X_test.shape[0])\n",
    "y_test1[999:1499]=-1\n",
    "y_scores=np.ones(X_test.shape[0])\n",
    "y_scores[(mse-tau)>0]=-1\n",
    "precision = precision_score(y_test1, y_scores)\n",
    "recall    = recall_score(y_test1, y_scores)\n",
    "accuracy = accuracy_score(y_test1, y_scores)\n",
    "f1 = f1_score(y_test1, y_scores, average='macro')\n",
    "print ('Tau : ', tau)\n",
    "print ('Precision : ', precision)\n",
    "print ('Recall: ', recall)\n",
    "print ('Accuracy : ', accuracy)\n",
    "print ('F1_score: ', f1)\n",
    "fig=plt.figure(4)\n",
    "red_dot, white_cross =plt.plot(mse, 'r--', np.ones(len(mse))*tau, 'b--')\n",
    "plt.xlabel('t')\n",
    "plt.ylabel('Anomaly scores')\n",
    "plt.legend([red_dot, (red_dot, white_cross)], [\"Anomaly scores\", \"Anomaly threshold\"])\n",
    "plt.show()\n",
    "fig.savefig('plot.png', dpi=1200)  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OCSVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#OCSVM\n",
    "e=X_train - predicted_train\n",
    "nsamples, nx, ny = e.shape\n",
    "d2_e = e.reshape((nsamples,nx*ny))\n",
    "\n",
    "from sklearn import svm\n",
    "clf = svm.OneClassSVM(nu=0.0055, kernel=\"rbf\", gamma=1.5)\n",
    "clf.fit(d2_e)\n",
    "\n",
    "e_t=X_test - predicted\n",
    "nsamples, nx, ny = e_t.shape\n",
    "d2_e_t = e_t.reshape((nsamples,nx*ny))\n",
    "y_scores = clf.predict(d2_e_t)\n",
    "precision = precision_score(y_test1, y_scores)\n",
    "recall    = recall_score(y_test1, y_scores)\n",
    "accuracy = accuracy_score(y_test1, y_scores)\n",
    "f1 = f1_score(y_test1, y_scores, average='macro')\n",
    "print ('Precision : ', precision)\n",
    "print ('Recall: ', recall)\n",
    "print ('Accuracy : ', accuracy)\n",
    "print ('F1_score: ', f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tran et al., 2019\n",
    "\n",
    "def build_model():\n",
    "    model = Sequential()\n",
    "    layers = {'input': 1, 'hidden1': 64, 'hidden2': 256, 'hidden3': 100, 'output': 1}\n",
    "\n",
    "    model.add(LSTM(\n",
    "            input_length=sequence_length - 1,\n",
    "            input_dim=layers['input'],\n",
    "            output_dim=layers['hidden1'],\n",
    "            return_sequences=True))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(LSTM(\n",
    "            layers['hidden2'],\n",
    "            return_sequences=True))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(LSTM(\n",
    "            layers['hidden3'],\n",
    "            return_sequences=False))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Dense(\n",
    "            output_dim=layers['output']))\n",
    "    model.add(Activation(\"linear\"))\n",
    "\n",
    "    start = time.time()\n",
    "    model.compile(loss=\"mse\", optimizer=\"rmsprop\")\n",
    "    print (\"Compilation Time : \", time.time() - start)\n",
    "    return model\n",
    "\n",
    "#KDE\n",
    "def kde_sklearn(x, bandwidth=0.2, **kwargs):\n",
    "\tx_grid = np.linspace(x.min() - 0.9*x.min(), x.max() + x.max(), 500)\n",
    "\t\"\"\"Kernel Density Estimation with Scikit-learn\"\"\"\n",
    "\tkde_skl = KernelDensity(bandwidth=bandwidth, **kwargs)\n",
    "\tkde_skl.fit(x[:, np.newaxis])\n",
    "\t# score_samples() returns the log-likelihood of the samples\n",
    "\tlog_pdf = kde_skl.score_samples(x_grid[:, np.newaxis])\n",
    "\treturn np.exp(log_pdf), x_grid\n",
    "   \n",
    "def FindThreshold(x,h,p):\n",
    "    tau=0\n",
    "    x.sort() \n",
    "    for i in range(len(x)):\n",
    "        int_K = integrate.quad(lambda s: (1/(h*np.sqrt(2*np.pi)))*np.exp(-0.5*(s-p)/h), (i-1)/len(x), i/len(x))\n",
    "        tau=tau+int_K[0]*x[i]\n",
    "    return tau\n",
    "  \n",
    "def run_network(model=None, data=None):\n",
    "    global_start_time = time.time()\n",
    "\n",
    "    if data is None:\n",
    "        print ('Loading data... ')\n",
    "        # train on first 700 samples and test on next 300 samples (has anomaly)\n",
    "        X_train,y_train, X_test, y_test = get_split_prep_data(0, 6999, 7000, 10000)\n",
    "    else:\n",
    "        X_train, y_train, X_test, y_test = data\n",
    "\n",
    "    print ('\\nData Loaded. Compiling...\\n')\n",
    "\n",
    "    if model is None:\n",
    "        model = build_model()\n",
    "\n",
    "    try:\n",
    "        print(\"Training...\")\n",
    "        model.fit(\n",
    "                X_train, y_train,\n",
    "                batch_size=batch_size, nb_epoch=epochs, validation_split=0.05)\n",
    "        print(\"Predicting...\")\n",
    "        predicted_train = model.predict(X_train)\n",
    "        predicted = model.predict(X_test)\n",
    "        print(\"Reshaping predicted\")\n",
    "        predicted_train = np.reshape(predicted_train, (predicted_train.size,))\n",
    "        predicted = np.reshape(predicted, (predicted.size,))\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"prediction exception\")\n",
    "        print ('Training duration (s) : ', time.time() - global_start_time)\n",
    "        return model, y_test, y_train, 0\n",
    "\n",
    "    try:\n",
    "        mse = ((y_test - predicted) ** 2)\n",
    "        plt.plot(mse, 'r')\n",
    "        mse_train = ((y_train - predicted_train) ** 2)\n",
    "        # use grid search cross-validation to optimize the bandwidth\n",
    "        params = {'bandwidth': np.linspace(0, 0.5, 10)}\n",
    "        grid = GridSearchCV(KernelDensity(), params, cv = 20)\n",
    "        grid.fit(mse_train[:, None])\n",
    "        print(\"best bandwidth: {0}\".format(grid.best_estimator_.bandwidth))\n",
    "        h=grid.best_estimator_.bandwidth\n",
    "        tau=FindThreshold(mse_train,h,0.56)\n",
    "        #tau=max(mse_train)\n",
    "        y_test1=np.ones(len(y_test))\n",
    "        y_test1[999:1499]=-1\n",
    "        y_scores=np.ones(len(y_test))\n",
    "        y_scores[(mse-tau)>0]=-1\n",
    "        \n",
    "        precision = precision_score(y_test1, y_scores)\n",
    "        recall    = recall_score(y_test1, y_scores)\n",
    "        accuracy = accuracy_score(y_test1, y_scores)\n",
    "        f1 = f1_score(y_test1, y_scores, average='macro') \n",
    "     \n",
    "        fig=plt.figure(3)\n",
    "        red_dot, white_cross =plt.plot(mse, 'r--', np.ones(len(mse))*tau, 'b--')\n",
    "        plt.xlabel('t')\n",
    "        plt.ylabel('Anomaly scores')\n",
    "        plt.legend([red_dot, (red_dot, white_cross)], [\"Anomaly scores\", \"Anomaly threshold\"])\n",
    "        plt.show()\n",
    "        fig.savefig('plot.png', dpi=1200)\n",
    "        print ('Tau : ', tau)\n",
    "        print ('Precision : ', precision)\n",
    "        print ('Recall: ', recall)\n",
    "        print ('Accuracy : ', accuracy)\n",
    "        print ('F1_score: ', f1)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(\"plotting exception\")\n",
    "        print (str(e))\n",
    "        print ('Training duration (s) : ', time.time() - global_start_time)\n",
    "\n",
    "    \n",
    "    \n",
    "    return model, y_test, predicted, y_train, predicted_train\n",
    "\n",
    "run_network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
